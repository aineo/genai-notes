{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNc4gYlQC6wD"
      },
      "source": [
        "###Check runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdkbPXWU19uD"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkV_0H5rjPXh"
      },
      "source": [
        "### Install packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzM3k9cOi-zy"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U transformers accelerate peft diffusers==0.32.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ycCoZhLjpck"
      },
      "source": [
        "### Check pytorch nightly installation for latest and fastest kernels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJAGtqQ3jtxv"
      },
      "outputs": [],
      "source": [
        "!pip install --pre torch --index-url https://download.pytorch.org/whl/nightly/cu121"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpPlCgXV2R3S"
      },
      "source": [
        "###Clear pipeline cash function\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hg041-rM2cE0"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import torch\n",
        "\n",
        "def clear_cache():\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()\n",
        "  torch.cuda.reset_peak_memory_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-dTR1uFk5t4"
      },
      "source": [
        "###Run with scaled dot product attention (SDPA)\n",
        "\n",
        "SDPA is an optimized and memory-efficient attention (similar to xFormers) that automatically enables several other optimizations depending on the model inputs and GPU type. SDPA is enabled by default if youâ€™re using PyTorch 2.0 and the latest version of ðŸ¤— Diffusers, so you donâ€™t need to add anything to your code.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5cKVbckBaJC"
      },
      "outputs": [],
      "source": [
        "from time import time\n",
        "from diffusers import DiffusionPipeline\n",
        "import torch\n",
        "\n",
        "# Load the pipeline in full-precision and place its model components on CUDA.\n",
        "pipe = DiffusionPipeline.from_pretrained(\n",
        "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "    torch_dtype=torch.bfloat16\n",
        ").to(\"cuda\")\n",
        "\n",
        "prompt = \"One horse in an aquarium, cold color palette, muted colors, detailed, 4k\"\n",
        "negative_prompt = \"bad anatomy, bad proportions, missed legs, ugly\"\n",
        "\n",
        "start = time()\n",
        "image = pipe(prompt, negative_prompt=negative_prompt, num_inference_steps=30).images[0]\n",
        "print('Inference time is {}'.format(time() - start))\n",
        "\n",
        "del pipe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xT_QaMx3DGg6"
      },
      "outputs": [],
      "source": [
        "image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAgyCnQMqA6E"
      },
      "source": [
        "###Run pipeline only with bfloat16\n",
        "\n",
        "There are several benefits of using reduced precision:\n",
        "\n",
        "*   Using a reduced numerical precision (such as float16 or bfloat16) for inference doesnâ€™t affect the generation quality but significantly improves latency.\n",
        "\n",
        "*   The benefits of using bfloat16 compared to float16 are hardware dependent, but modern GPUs tend to favor bfloat16.\n",
        "\n",
        "*   bfloat16 is much more resilient when used with quantization compared to float16, but more recent versions of the quantization library (torchao) we used donâ€™t have numerical issues with float16."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2BeZat3lmOc"
      },
      "outputs": [],
      "source": [
        "clear_cache()\n",
        "\n",
        "# Load the pipeline in full-precision and place its model components on CUDA.\n",
        "pipe = DiffusionPipeline.from_pretrained(\n",
        "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "    torch_dtype=torch.bfloat16\n",
        ").to(\"cuda\")\n",
        "\n",
        "# Run the attention ops without SDPA.\n",
        "pipe.unet.set_default_attn_processor()\n",
        "pipe.vae.set_default_attn_processor()\n",
        "\n",
        "start = time()\n",
        "image = pipe(prompt, negative_prompt=negative_prompt, num_inference_steps=30).images[0]\n",
        "print('Inference time is {}s'.format(time() - start))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xM9JoYc4DJX6"
      },
      "outputs": [],
      "source": [
        "image"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
